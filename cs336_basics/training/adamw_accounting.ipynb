{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f269c46b",
   "metadata": {},
   "source": [
    "# How much peak memory does running AdamW require?\n",
    "\n",
    "Recall e2e transformer flow:\n",
    "\n",
    "```\n",
    "Input:\n",
    "  x: (B, T, d_model)\n",
    "\n",
    "For each transformer block:\n",
    "  x_norm = RMSNorm(x)\n",
    "\n",
    "  # Attention\n",
    "  QKV = x_norm @ W_QKV\n",
    "  Q,K,V → reshape to (B, H, T, d_head)\n",
    "  attn = softmax(QKᵀ / √d_head + mask)\n",
    "  out = attn @ V\n",
    "  out = concat_heads(out) @ W_O\n",
    "  x = x + out\n",
    "\n",
    "  # MLP\n",
    "  x_norm = RMSNorm(x)\n",
    "  u = x_norm @ W_up\n",
    "  v = x_norm @ W_gate\n",
    "  out = swish(v) ⊙ u @ W_down\n",
    "  x = x + out\n",
    "\n",
    "Final:\n",
    "  x = RMSNorm(x)\n",
    "  logits = x @ W_vocab\n",
    "```\n",
    "\n",
    "AdamW per-param cost:\n",
    "* 1st moment: 3 flops (m = b1*m + (1-b1)g => 2 mult, 1 add)\n",
    "* 2nd moment: 3 flops\n",
    "* param update: 5 flops\n",
    "* param decay: 2 flop\n",
    "Overall roughly 10x FLOPs per param.\n",
    "\n",
    "Now looking into a single transformer block\n",
    "1. RMS norm: d_model params\n",
    "2. MHA: d_model * d_model params\n",
    "3. SWIGLU: d_ff * d_model = 4*d_model^2 params\n",
    "\n",
    "And also after transformer blocks:\n",
    "1. final RMS norm: d_model params\n",
    "2. output embedding: d_model * d_vocab\n",
    "3. cross-entropy on logits: d_vocab\n",
    "\n",
    "So total num params is:\n",
    "\n",
    "> num_layers * (d_model + 5*d_model^2) + d_model + d_model * d_vocab + d_vocab\n",
    "\n",
    "RAM cost:\n",
    "\n",
    "AdamW requires storing two extra tensors per parameter (m and v), so optimizer state alone costs ~2x model size. Including gradients, total parameter-related memory is ~4x num_params. Peak training memory is usually dominated by activations (i.e. intermediate tensors that must be saved for gradient calculations, e.g. the x's), not AdamW.\n",
    "\n",
    "Note that although intuitively, x is a vector and W is a matrix and this feels W is more expensive, in practice it's really batch * seq_len number of x vectors, vs. a d_model^2 parameter W matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c1bed",
   "metadata": {},
   "source": [
    "# Instantiate your answer for a GPT-2 XL-shaped model to get an expression that only depends on the batch_size. What is the maximum batch size you can use and still fit within 80GB memory?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4624e803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2445f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford-cs336",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
